{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __MLP-Mixer__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import torch \n",
    "import torchvision \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns \n",
    "\n",
    "from torchvision.transforms import v2 \n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will again be implementing our __MLP-Mixer__ model on __CIFAR10__ dataset. This dataset has $60,000$ images of size $32 \\times 32$ in 10 classes with $6000$ image per class, so fairly balanced. This dataset, in addition, is already divided into $50,000$ training and $10,000$ test images. \n",
    "\n",
    "We will split $5,000$ off of training to create a validation set, leaving training set with $45,000$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#getting test_set first to capture mean and std\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "root = \"./data\"\n",
    "test_set = CIFAR10(root=root, train=False, download=True, transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_std(dataset):\n",
    "    \"\"\"\"\n",
    "    To compute Mean and Standard Deviation for each channel\n",
    "    \"\"\"\n",
    "\n",
    "    mean = torch.zeros(3)  \n",
    "    std = torch.zeros(3)   \n",
    "    total_pixels = 0\n",
    "\n",
    "    for image, _ in dataset:\n",
    "        image = v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])(image)  # Converting images to tensors\n",
    "        image = image.view(3, -1)  # Flattening the image to [3, height*width]\n",
    "        mean += image.sum(dim=1)   # getting the sum of pixel values for each channel\n",
    "        std += (image ** 2).sum(dim=1)  #getting the sum of squared pixel values for each channel\n",
    "        total_pixels += image.size(1)\n",
    "\n",
    "    # Computing the mean and std across the entire dataset\n",
    "    mean /= total_pixels\n",
    "    std = torch.sqrt(std / total_pixels - (mean ** 2))\n",
    "\n",
    "    return mean.tolist(), std.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting Mean and Standard Deviation from earlier defined function, \n",
    "# and apply transformation using it through Normalize and other augmentation\n",
    "\n",
    "mean, std = compute_mean_std(test_set)\n",
    "\n",
    "train_transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.RandomHorizontalFlip(),\n",
    "    v2.RandomResizedCrop((32, 32), scale=(0.8, 1.0), ratio=(0.9, 1.1)),\n",
    "    v2.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "test_transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=mean, std=std)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading training set as well, separating validation set out of training set, appling augmentations, and finally creating DATALOADER for each subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_set = CIFAR10(root=root, train=True, transform=train_transform, download=True)\n",
    "\n",
    "#again same for validation set, this is to different augmentatio application \n",
    "validation_set = CIFAR10(root=root, train=True, transform=test_transform, download=True)\n",
    "test_set = CIFAR10(root=root, train=False, transform=test_transform, download=True)\n",
    "\n",
    "#separating out distinct train_set and validation set\n",
    "train_set, _ = torch.utils.data.random_split(train_set, [45000, 5000],generator=torch.Generator().manual_seed(42))\n",
    "_, validation_set = torch.utils.data.random_split(validation_set, [45000, 5000], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "\n",
    "#Next, creating DataLoader(s)\n",
    "train_loader = DataLoader(train_set, batch_size=128, shuffle=True, drop_last=True,num_workers=2, pin_memory=True)\n",
    "validation_loader = DataLoader(validation_set, batch_size=128, shuffle=True, drop_last=False ,num_workers=2)\n",
    "test_loader = DataLoader(test_set, batch_size=128, shuffle=True, drop_last=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
